{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading Data\n",
    "dataset = load_dataset('lukebarousse/data_jobs')\n",
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "# Data Cleanup\n",
    "df['job_posted_date'] = pd.to_datetime(df['job_posted_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d13e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from datasets) (2.1.3)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.3.2)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: anyio in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ishaa\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.2.0-py3-none-any.whl (506 kB)\n",
      "Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "   ---------------------------------------- 0.0/564.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 564.3/564.3 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-21.0.0-cp313-cp313-win_amd64.whl (26.1 MB)\n",
      "   ---------------------------------------- 0.0/26.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 3.4/26.1 MB 22.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 5.2/26.1 MB 12.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 8.4/26.1 MB 12.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 11.3/26.1 MB 13.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 13.6/26.1 MB 13.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 16.8/26.1 MB 13.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.9/26.1 MB 13.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.5/26.1 MB 13.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 24.1/26.1 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.0/26.1 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.1/26.1 MB 12.0 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.6.0-cp313-cp313-win_amd64.whl (31 kB)\n",
      "Installing collected packages: xxhash, pyarrow, multiprocess, huggingface-hub, datasets\n",
      "\n",
      "  Attempting uninstall: pyarrow\n",
      "\n",
      "    Found existing installation: pyarrow 19.0.0\n",
      "\n",
      "    Uninstalling pyarrow-19.0.0:\n",
      "\n",
      "   -------- ------------------------------- 1/5 [pyarrow]\n",
      "      Successfully uninstalled pyarrow-19.0.0\n",
      "   -------- ------------------------------- 1/5 [pyarrow]\n",
      "   -------- ------------------------------- 1/5 [pyarrow]\n",
      "   -------- ------------------------------- 1/5 [pyarrow]\n",
      "   -------- ------------------------------- 1/5 [pyarrow]\n",
      "   -------- ------------------------------- 1/5 [pyarrow]\n",
      "   -------- ------------------------------- 1/5 [pyarrow]\n",
      "   -------- ------------------------------- 1/5 [pyarrow]\n",
      "   -------- ------------------------------- 1/5 [pyarrow]\n",
      "   -------- ------------------------------- 1/5 [pyarrow]\n",
      "   -------- ------------------------------- 1/5 [pyarrow]\n",
      "   -------- ------------------------------- 1/5 [pyarrow]\n",
      "   -------- ------------------------------- 1/5 [pyarrow]\n",
      "   -------- ------------------------------- 1/5 [pyarrow]\n",
      "   ---------------- ----------------------- 2/5 [multiprocess]\n",
      "   ---------------- ----------------------- 2/5 [multiprocess]\n",
      "   ------------------------ --------------- 3/5 [huggingface-hub]\n",
      "   ------------------------ --------------- 3/5 [huggingface-hub]\n",
      "   ------------------------ --------------- 3/5 [huggingface-hub]\n",
      "   ------------------------ --------------- 3/5 [huggingface-hub]\n",
      "   ------------------------ --------------- 3/5 [huggingface-hub]\n",
      "   -------------------------------- ------- 4/5 [datasets]\n",
      "   -------------------------------- ------- 4/5 [datasets]\n",
      "   -------------------------------- ------- 4/5 [datasets]\n",
      "   -------------------------------- ------- 4/5 [datasets]\n",
      "   -------------------------------- ------- 4/5 [datasets]\n",
      "   ---------------------------------------- 5/5 [datasets]\n",
      "\n",
      "Successfully installed datasets-4.2.0 huggingface-hub-0.35.3 multiprocess-0.70.16 pyarrow-21.0.0 xxhash-3.6.0\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f464531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a0f44afeb045bca3815a54d0911a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishaa\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ishaa\\.cache\\huggingface\\hub\\datasets--lukebarousse--data_jobs. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b7087858434af98ceb43360ea3bcf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_jobs.csv:   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76409c954fde49f18efa385dcefb09b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/785741 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading Data\n",
    "dataset = load_dataset('lukebarousse/data_jobs')\n",
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "# Data Cleanup\n",
    "df['job_posted_date'] = pd.to_datetime(df['job_posted_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c28fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Clinical Data Engineer / Principal Clin...</td>\n",
       "      <td>Watertown, CT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Guadalajara, Jalisco, Mexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Engineer/Scientist/Analyst, Mid or Senior...</td>\n",
       "      <td>Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LEAD ENGINEER - PRINCIPAL ANALYST - PRINCIPAL ...</td>\n",
       "      <td>San Antonio, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer- Sr Jobs</td>\n",
       "      <td>Washington, DC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title  \\\n",
       "0  Senior Clinical Data Engineer / Principal Clin...   \n",
       "1                                       Data Analyst   \n",
       "2  Data Engineer/Scientist/Analyst, Mid or Senior...   \n",
       "3  LEAD ENGINEER - PRINCIPAL ANALYST - PRINCIPAL ...   \n",
       "4                             Data Engineer- Sr Jobs   \n",
       "\n",
       "                   job_location  \n",
       "0                 Watertown, CT  \n",
       "1  Guadalajara, Jalisco, Mexico  \n",
       "2               Berlin, Germany  \n",
       "3               San Antonio, TX  \n",
       "4                Washington, DC  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('lukebarousse/data_jobs')\n",
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "df.iloc[:5, [1, 2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54bfc51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_via</th>\n",
       "      <th>job_schedule_type</th>\n",
       "      <th>job_work_from_home</th>\n",
       "      <th>search_location</th>\n",
       "      <th>job_posted_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GCP Data Engineer</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>via ZipRecruiter</td>\n",
       "      <td>Contractor and Temp work</td>\n",
       "      <td>True</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>2023-11-07 14:01:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Engineer  - GCP Cloud</td>\n",
       "      <td>Dearborn, MI</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>Florida, United States</td>\n",
       "      <td>2023-03-27 13:18:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>True</td>\n",
       "      <td>Romania</td>\n",
       "      <td>2023-12-07 13:40:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Technology &amp; Operations Business Analyst</td>\n",
       "      <td>Copenhagen, Denmark</td>\n",
       "      <td>via Trabajo.org</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>2023-06-05 13:44:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist II</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>via ZipRecruiter</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>True</td>\n",
       "      <td>New York, United States</td>\n",
       "      <td>2023-04-23 13:02:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  job_title         job_location  \\\n",
       "5                         GCP Data Engineer             Anywhere   \n",
       "6         Senior Data Engineer  - GCP Cloud         Dearborn, MI   \n",
       "7                             Data Engineer             Anywhere   \n",
       "8  Technology & Operations Business Analyst  Copenhagen, Denmark   \n",
       "9                         Data Scientist II             Anywhere   \n",
       "\n",
       "            job_via         job_schedule_type  job_work_from_home  \\\n",
       "5  via ZipRecruiter  Contractor and Temp work                True   \n",
       "6      via LinkedIn                 Full-time               False   \n",
       "7      via LinkedIn                 Full-time                True   \n",
       "8   via Trabajo.org                 Full-time               False   \n",
       "9  via ZipRecruiter                 Full-time                True   \n",
       "\n",
       "           search_location      job_posted_date  \n",
       "5                  Georgia  2023-11-07 14:01:59  \n",
       "6   Florida, United States  2023-03-27 13:18:18  \n",
       "7                  Romania  2023-12-07 13:40:49  \n",
       "8                  Denmark  2023-06-05 13:44:34  \n",
       "9  New York, United States  2023-04-23 13:02:57  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5:9, 'job_title':'job_posted_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e64ebccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['job_title_short', 'job_title', 'job_location', 'job_via', 'job_schedule_type', 'job_work_from_home', 'search_location', 'job_posted_date', 'job_no_degree_mention', 'job_health_insurance', 'job_country', 'salary_rate', 'salary_year_avg', 'salary_hour_avg', 'company_name', 'job_skills', 'job_type_skills'],\n",
      "        num_rows: 785741\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5ed96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary_year_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Clinical Data Engineer / Principal Clin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Engineer/Scientist/Analyst, Mid or Senior...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LEAD ENGINEER - PRINCIPAL ANALYST - PRINCIPAL ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer- Sr Jobs</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GCP Data Engineer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Engineer  - GCP Cloud</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Technology &amp; Operations Business Analyst</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist II</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Erfahrene*r Data Engineer*in (m/w/d)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Stagiaire Data Analyst (H/F) - Lyon (69006)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            job_title  salary_year_avg\n",
       "0   Senior Clinical Data Engineer / Principal Clin...              NaN\n",
       "1                                        Data Analyst              NaN\n",
       "2   Data Engineer/Scientist/Analyst, Mid or Senior...              NaN\n",
       "3   LEAD ENGINEER - PRINCIPAL ANALYST - PRINCIPAL ...              NaN\n",
       "4                              Data Engineer- Sr Jobs              NaN\n",
       "5                                   GCP Data Engineer              NaN\n",
       "6                   Senior Data Engineer  - GCP Cloud              NaN\n",
       "7                                       Data Engineer              NaN\n",
       "8            Technology & Operations Business Analyst              NaN\n",
       "9                                   Data Scientist II              NaN\n",
       "10               Erfahrene*r Data Engineer*in (m/w/d)              NaN\n",
       "11                                      Data Engineer              NaN\n",
       "12        Stagiaire Data Analyst (H/F) - Lyon (69006)              NaN\n",
       "13                               Senior Data Engineer              NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['job_country'] == 'United States']\n",
    "df.iloc[:14, [1, 12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafdd078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
